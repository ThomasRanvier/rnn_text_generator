{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN to generate text from GOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 5662324 characters\n",
      "\n",
      "\n",
      "“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\n",
      "\n",
      "“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\n",
      "\n",
      "Gared did not rise to the bait. He was an old man, past f\n",
      "86 unique characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = open('gameofthrones.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  'f' :  55,\n",
      "  '—' :  80,\n",
      "  'z' :  75,\n",
      "  '“' :  83,\n",
      "  '/' :   8,\n",
      "  'Z' :  47,\n",
      "  ' ' :   1,\n",
      "  'H' :  29,\n",
      "  '1' :  10,\n",
      "  'K' :  32,\n",
      "  'b' :  51,\n",
      "  'q' :  66,\n",
      "  '’' :  82,\n",
      "  'X' :  45,\n",
      "  'W' :  44,\n",
      "  'E' :  26,\n",
      "  'm' :  62,\n",
      "  'x' :  73,\n",
      "  '8' :  17,\n",
      "  '!' :   2,\n",
      "  ...\n",
      "}\n",
      "'\\n\\n“We should ' ---- characters mapped to int ---- > [ 0  0 83 44 54  1 68 57 64 70 61 53  1]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {char : index for index, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "'\\n\\n“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\\n\\n“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\\n\\nGared di'\n",
      "'d not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. “Dead is dead,” he said. “We have no business with the dead.”\\n\\n“Are they dead?” Royce asked softly. “Wh'\n",
      "'at proof have we?”\\n\\n“Will saw them,” Gared said. “If he says they are dead, that’s proof enough for me.”\\n\\nWill had known they would drag him into the quarrel sooner or later. He wished it had been late'\n",
      "'r rather than sooner. “My mother told me that dead men sing no songs,” he put in.\\n\\n“My wet nurse said the same thing, Will,” Royce replied. “Never believe anything you hear at a woman’s tit. There are '\n",
      "'things to be learned even from the dead.” His voice echoed, too loud in the twilit forest.\\n\\n“We have a long ride before us,” Gared pointed out. “Eight days, maybe nine. And night is falling.”\\n\\nSer Waym'\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 200\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "# Creates a Dataset whose elements are slices of the given tensors.\n",
    "\n",
    "# Splits the dataset in batches of size seq_length\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: '\\n\\n“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\\n\\n“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\\n\\nGared d'\n",
      "Target data:'\\n“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\\n\\n“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\\n\\nGared di'\n"
     ]
    }
   ],
   "source": [
    "# Generate the input and target texts\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "  print ('Input data: ' + repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:' + repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    1\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 83 ('“')\n",
      "Step    2\n",
      "  input: 83 ('“')\n",
      "  expected output: 44 ('W')\n",
      "Step    3\n",
      "  input: 44 ('W')\n",
      "  expected output: 54 ('e')\n",
      "Step    4\n",
      "  input: 54 ('e')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "# Goes through the 5 first steps of the first sequence\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 200), (64, 200)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True), \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=len(vocab), \n",
    "    embedding_dim=embedding_dim, \n",
    "    rnn_units=rnn_units, \n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 200, 86) # (batch_size, sequence_length, vocab_size)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22016     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 86)            88150     \n",
      "=================================================================\n",
      "Total params: 4,045,398\n",
      "Trainable params: 4,045,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-0a804a16f480>:2: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "Input: \n",
      "'sailed ages ago.\\n\\nTwo of the guardsmen were dicing together while the third walked rounds, his hand on the pommel of his sword. Ashamed to let them see her crying like a baby, she stopped to rub at he'\n",
      "\n",
      "Next Char Predictions: \n",
      "'KD)6xn:’edêklaruJNP-Hgh30lA5s[iPU5mCM]u9fjwebWGYHV(6kZMzOlLW‘r-Z:KS8F‘}6h}5f.3uo)zs}(0IVtv)SpE—[]ZmV”lOyHu8fk6THDOx6,!ps‘C\\nQ:zLJ 9ytK-I5f0((?Jdf3…WucY“tL/—c69lN/cYn:cF]wJ.{Y5kr-m)ns(Pva F5LEi9R4?k —Bn'\n"
     ]
    }
   ],
   "source": [
    "# Decode the prediction of the model\n",
    "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print('Input: \\n' + repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print('')\n",
    "print('Next Char Predictions: \\n' + repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (64, 200, 86) # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:      4.4535155\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \" + str(example_batch_predictions.shape) + \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \" + str(example_batch_loss.numpy().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.train.AdamOptimizer(), loss = loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "441/442 [============================>.] - ETA: 0s - loss: 2.1226WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "442/442 [==============================] - 106s 240ms/step - loss: 2.1213\n",
      "Epoch 2/15\n",
      "442/442 [==============================] - 103s 234ms/step - loss: 1.4204\n",
      "Epoch 3/15\n",
      "442/442 [==============================] - 104s 234ms/step - loss: 1.2714\n",
      "Epoch 4/15\n",
      "442/442 [==============================] - 103s 234ms/step - loss: 1.2057\n",
      "Epoch 5/15\n",
      "442/442 [==============================] - 103s 232ms/step - loss: 1.1651\n",
      "Epoch 6/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 1.1347\n",
      "Epoch 7/15\n",
      "442/442 [==============================] - 104s 234ms/step - loss: 1.1094\n",
      "Epoch 8/15\n",
      "442/442 [==============================] - 103s 232ms/step - loss: 1.0885\n",
      "Epoch 9/15\n",
      "442/442 [==============================] - 103s 234ms/step - loss: 1.0694\n",
      "Epoch 10/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 1.0521\n",
      "Epoch 11/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 1.0364\n",
      "Epoch 12/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 1.0218\n",
      "Epoch 13/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 1.0083\n",
      "Epoch 14/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 0.9969\n",
      "Epoch 15/15\n",
      "442/442 [==============================] - 103s 233ms/step - loss: 0.9858\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=15\n",
    "\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22016     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 86)             88150     \n",
      "=================================================================\n",
      "Total params: 4,045,398\n",
      "Trainable params: 4,045,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=1000):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jon reminded herself trying to open the surface of the feast meals. They are worse than you can believe that the steward serve me, she told herself.\n",
      "\n",
      "Three hundred he had liked. She says with this falseheld smile as I do, Bran. Brother. The master of cities all night, Jon?”\n",
      "\n",
      "“Not that I suffered that or will she broken you, nepreed. Lord Alester said he needed to hear after it is, blue as bruise.”\n",
      "\n",
      "Sansa seurting the dripped face, he plurped the brill as he loosed. Snow might have killed anything I had to kill him. He had no time for a fawn burning beside this damned black-girl ring, trade three-hundred red eyes, and use was someone else more auling another she stones that night.\n",
      "\n",
      "He had been watching him, and to men he would have the peffice of Lord Eddard, unnisted to spy at her wrist. “Your crimes,” she told them. “Bastard of sixter, you did nothank your father’s heir, an axe ill.”\n",
      "\n",
      "Ned had taken how it began to smile. “Have you ever wanted anyn called. They send no matter what they ha\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'Jon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robberch of the Seven Kingdoms.” The knight stroked her cheek cletched and galloping loose sourleaf of the riderlass. It was hard to take his meat hissing, not wet and awkward, but Littlefinger had been ribbong up, and even the biggest she was… Cersei will like to kill her?”\n",
      "\n",
      "Dany could not have called him as he chase of him.\n",
      "\n",
      "The black cloak was shr fierce that Sam had a moment, lean and among their laps. “I shall look in his grief, his fighting wailed to the core from the mouth to following.\n",
      "\n",
      "“Four here will your chosen you won’t be safe for Ned Stark, very kind. What come beyond crows, from the roof of the realm. We’re left.”\n",
      "\n",
      "“I didn’t serve huntry in King’s Landing,” the boy said. “Whe wants the desert out where I am done, to present you that he is my brother, Ser were you. Seven are coming on!”\n",
      "\n",
      "Khal Drogo grumbled at the morning archers, as if tringing Tyrion had horses.\n",
      "\n",
      "They jumped up out of the castle wall, clad in anxious was half-catched by camp followers, and when they were ta\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'Robb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eddard said, “Ser Ryman, then men are broken, not trying not, and ride back to Asshai, ser.”\n",
      "\n",
      "“In?” he said. “That was said, and the Hands come to King Robert’s Rebellion, for the man’s scars.\n",
      "\n",
      "At first, the pursue was likely till now? I thought they do not like they, the prayers. Ser Red cleared rain and waggled plate. With Ser Whalen Frey Tyrell s of darkness beneath the reddalt, teeth stretched before long sharp sun silences. After the size Lannister gathered before the vast finger was pale orange icorrides, if they were not every little man in a stick.\n",
      "\n",
      "“Nothing to fight these pleasures. And yet… cut off. Even their corpses might teach me what you need to call orders. Renly rather peetle flinking into Lord White Harbor, and Unsullied in the Seven Kingdoms, she thought she had killed her sister. She turned to Dater a killer. “I shall die in my eyes…” Bores were left. “Will I miss mine.”\n",
      "\n",
      "“Your return…” Jon admitted, staring back. “I have wrapped your tent back to the morrow, or watch me i\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'Eddard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cersei understood that Khal Drogo’s flesh… they are convenient ago. His lordship was welcome to her, but the mea-they’s breath saw Kraznys thick sullen stone of the mast, golden embroidered with soft, alongside casules, walls and armor, dangerous, the window, we are holding maidens and listen to overhousands. “Thould take some merchant innocent right take now?”\n",
      "\n",
      "Sansa loaned his head against a few mounted with red. They gave her at them and said, “We are not telling her?”\n",
      "\n",
      "“Smoke to the night, him and children but the snow. Neither men have the less, and eaten.”\n",
      "\n",
      "“She is my master’ every day after me,” he said.\n",
      "\n",
      "Arya was pulling after her; Varys was failing, his own hand clenched.\n",
      "\n",
      "“Where she told Davos we’ve hair and seed, whether she was heard,” declared Lord Tywin. “So you will wish I could…”\n",
      "\n",
      "“She looked up with Prince Tommen, when she saw the gate shadow from its exterior. They someteard is not there. Ser Jorah Mormont had smashed past her men amused herself. “This is a father ouldn’t!”\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'Cersei'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wildling, his cheeks were cragged at the body. “Shadows is gone and the father told me, when it is the sea, his castle will be wide behind his breasts. “The shadows come to Riverrun,” the Lord Commander offered softly. “Maybe Robb, the venisan Drogo is rich and rich in the gallery. Ser Loras is innocent, Father? Perhaps I should do with you. You request spells to frog a maester’s cony. Evin there is nothing.”\n",
      "\n",
      "Tyrion shook his head. “Or my man-ons,” the man made a face of Rattleshirt room as his mouth trying to feel Ser Meryn Trant sensed than ten, fortyffinger came down on him, but there wine and the heavy lank where she heard it music layer, past ith thick were cracked and stroked her eyes to his breath when Arya licked her away. “Jory wad Robb and after Lysa Arryn.”\n",
      "\n",
      "“The queen fears your new?”\n",
      "\n",
      "“I am his father’s aple, Kingslayer.”\n",
      "\n",
      "“They hear me lide,” he said.\n",
      "\n",
      "“I’m afraid I desperate wrought and let this owen than Varys. All about him, he had to look so little, and why, not if he suscer\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'wildling'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kingslayer, and in his mother, LIrd Walder Freys are let his shoes seem to warm her accustoms,” Rhoen said. “You won’t have the Lady Brienne.”\n",
      "\n",
      "“If you were lacion or snarks, men will enders will rule the bravos to return in shattered swords.\n",
      "\n",
      "Dany paused to swing it. The pounding, the only thing that Blowfore suitseated the Blackfish. He did not have the council the or they’d provek a few withered armor.”\n",
      "\n",
      "“Sad lie. Toge I’ll make a hundred scothe is,” Ser Rodrik commented sullenly to break the cup at Duskendale.\n",
      "\n",
      "“I didn’t keep him now.”.\n",
      "\n",
      "Two beasts were pointing through arrows along the blankets, drawing some blood to long aforonous while I needed every visits of yours.” Ten, how it hadn’t been sold from that god no party and hear, not carefully. Only then did they have soon enough to struggle when they made good nightful. Gods did not really know Jon. Not crushing Dolorous Edd Satin. As his and her name can sit, he knew it so not there must know. Your ransom walked off the gates. Their word\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u'Kingslayer'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
